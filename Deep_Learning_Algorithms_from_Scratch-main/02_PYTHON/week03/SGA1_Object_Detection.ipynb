{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "SGA1_Object Detection.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MaSHiIyGd23"
   },
   "source": [
    "# Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcPTRljhXhhI"
   },
   "source": [
    "In this assignment, you will implement a fruit detector. \n",
    "The task is divided into steps for simpler navigation.\n",
    "\n",
    "Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fcggFBc_FdC2"
   },
   "source": [
    "# we will need this library to process the labeling\n",
    "! pip install xmltodict"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HsTOiuplEugf"
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import xmltodict\n",
    "import json\n",
    "import glob\n",
    "import cv2\n",
    "import os\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfbyE1_tIL1x"
   },
   "source": [
    "## Step 0. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgTskp7RbqFZ"
   },
   "source": [
    "First, let's load the data that you can download [here](https://drive.google.com/file/d/1Ve5e9qdy_sUCMM4qXWrw8ecURg2af9Cm/view?usp=sharing). \n",
    "\n",
    "We have already written a dataset class for you and we encourage you to figure out how it works."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LelLza0-FR_Y"
   },
   "source": [
    "class2tag = {\"apple\": 1, \"orange\": 2, \"banana\": 3}\n",
    "\n",
    "\n",
    "class FruitDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.images = []\n",
    "        self.annotations = []\n",
    "        self.transform = transform\n",
    "        for annotation in glob.glob(data_dir + \"/*xml\"):\n",
    "            image_fname = os.path.splitext(annotation)[0] + \".jpg\"\n",
    "            self.images.append(cv2.cvtColor(cv2.imread(image_fname), cv2.COLOR_BGR2RGB))\n",
    "            with open(annotation) as f:\n",
    "                annotation_dict = xmltodict.parse(f.read())\n",
    "            bboxes = []\n",
    "            labels = []\n",
    "            objects = annotation_dict[\"annotation\"][\"object\"]\n",
    "            if not isinstance(objects, list):\n",
    "                objects = [objects]\n",
    "            for obj in objects:\n",
    "                bndbox = obj[\"bndbox\"]\n",
    "                bbox = [bndbox[\"xmin\"], bndbox[\"ymin\"], bndbox[\"xmax\"], bndbox[\"ymax\"]]\n",
    "                bbox = list(map(int, bbox))\n",
    "                bboxes.append(torch.tensor(bbox))\n",
    "                labels.append(class2tag[obj[\"name\"]])\n",
    "            self.annotations.append(\n",
    "                {\"boxes\": torch.stack(bboxes).float(), \"labels\": torch.tensor(labels)}\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.transform:\n",
    "            # the following code is correct if you use albumentations\n",
    "            # if you use torchvision transforms you have to modify it\n",
    "            res = self.transform(\n",
    "                image=self.images[i],\n",
    "                bboxes=self.annotations[i][\"boxes\"],\n",
    "                labels=self.annotations[i][\"labels\"],\n",
    "            )\n",
    "            return res[\"image\"], {\n",
    "                \"boxes\": torch.tensor(res[\"bboxes\"]),\n",
    "                \"labels\": torch.tensor(res[\"labels\"]),\n",
    "            }\n",
    "        else:\n",
    "            return self.images[i], self.annotations[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OCL52YoiigR"
   },
   "source": [
    "## Step 1. Intersection over Union (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bsybDfSlIrFL"
   },
   "source": [
    "In the [Object Detection task](https://en.wikipedia.org/wiki/Object_detection), you need to find objects of a certain class on the image and locate their positions (using the bounding box). The  model should predict the coordinates of the bounding box `[x0, y0, x1, y1]` and the label for this box. The model can predict multiple candidate bounding boxes for an object. We will select candidates using [Intersection Over Union](https://en.wikipedia.org/wiki/Jaccard_index)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otUTl7TMi6Ts"
   },
   "source": [
    "<img src=https://upload.wikimedia.org/wikipedia/commons/c/c7/Intersection_over_Union_-_visual_equation.png>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPZqTTOgjX_K"
   },
   "source": [
    "Implement a function that will calculate IoU for bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gtCPABU8dXOz"
   },
   "source": [
    "def intersection_over_union(dt_bbox, gt_bbox):\n",
    "    \"\"\"\n",
    "    Intersection over Union between two bboxes\n",
    "    :param dt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
    "    :param gt_bbox: list or numpy array of size (4,) [x0, y0, x1, y1]\n",
    "    :return : intersection over union\n",
    "    \"\"\"\n",
    "    ## YOUR CODE HERE\n",
    "    \n",
    "    return iou"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2XtWBs9jq1I"
   },
   "source": [
    "If the function is implemented correctly, then the execution of the following cell will produce:\n",
    "\n",
    "**0.14285714285714285**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wXZFFOTBjjBX"
   },
   "source": [
    "dt_bbox = [0, 0, 2, 2]\n",
    "gt_bbox = [1, 1, 3, 3]\n",
    "intersection_over_union(dt_bbox, gt_bbox)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Loadw2Krkq_a"
   },
   "source": [
    "## Step 2. Evaluate Sample (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIrVulAQLidT"
   },
   "source": [
    "We now have to evaluate the predictions of the model. To do this, we will write a function that will do the following:\n",
    "1. Take model predictions and ground truth bounding boxes and labels as inputs.\n",
    "2. For each bounding box from the prediction, find the closest bounding box among the answers.\n",
    "3. For each found pair of bounding boxes, check whether the IoU is greater than a certain threshold `iou_threshold`. If the **IoU** exceeds the threshold, then we consider this answer as **True Positive**.\n",
    "4. Remove a matched bounding box from the evaluation.\n",
    "5. For each predicted bounding box, return the detection score and whether we were able to match it or not."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oK54_evGzyQg"
   },
   "source": [
    "def evaluate_sample(target_pred, target_true, iou_threshold=0.5):\n",
    "    # ground truth\n",
    "    gt_bboxes = target_true['boxes'].numpy()\n",
    "    gt_labels = target_true['labels'].numpy()\n",
    "\n",
    "    # predictions\n",
    "    dt_bboxes = target_pred['boxes'].numpy()\n",
    "    dt_labels = target_pred['labels'].numpy()\n",
    "    dt_scores = target_pred['scores'].numpy()\n",
    "\n",
    "    results = []\n",
    "    # for each bounding box from the prediction, find the closest bounding box among the answers\n",
    "    for detection_id in range(len(dt_labels)):\n",
    "        dt_bbox = dt_bboxes[detection_id, :]\n",
    "        dt_label = dt_labels[detection_id]\n",
    "        dt_score = dt_scores[detection_id]\n",
    "\n",
    "        detection_result_dict = {'score': dt_score}\n",
    "\n",
    "        ## YOUR CODE HERE\n",
    "        \n",
    "        if max_gt_id >= 0 and max_IoU >= iou_threshold:\n",
    "            # mark as True Positive\n",
    "            detection_result_dict['TP'] = 1\n",
    "            # delete matched bounding box\n",
    "            gt_labels = np.delete(gt_labels, max_gt_id, axis=0)\n",
    "            gt_bboxes = np.delete(gt_bboxes, max_gt_id, axis=0)\n",
    "\n",
    "        else:\n",
    "            detection_result_dict['TP'] = 0\n",
    "\n",
    "        results.append(detection_result_dict)\n",
    "\n",
    "    return results"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wgZX7BqUPUtk"
   },
   "source": [
    "## Step 3. Evaluate Model (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5eWfC29RCKJ"
   },
   "source": [
    "To assess the quality of the model, we will use the [mAP](https://jonathan-hui.medium.com/\\map-mean-average-precision-for-object-detection-45c121a31173) metric defined as AP Area under the curve. To do this, you will need to calculate `recall` and` precision`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WK04btkgQAq3"
   },
   "source": [
    "from sklearn.metrics import auc"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jhmZOkQajpwZ"
   },
   "source": [
    "def evaluate(model, test_loader, device):\n",
    "    results = []\n",
    "    model.eval()\n",
    "    nbr_boxes = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (images, targets_true) in enumerate(test_loader):\n",
    "            images = list(image.to(device).float() for image in images)\n",
    "            targets_pred = model(images)\n",
    "\n",
    "            targets_true = [{k: v.cpu().float() for k, v in t.items()} for t in targets_true]\n",
    "            targets_pred = [{k: v.cpu().float() for k, v in t.items()} for t in targets_pred]\n",
    "\n",
    "            for i in range(len(targets_true)):\n",
    "                target_true = targets_true[i]\n",
    "                target_pred = targets_pred[i]\n",
    "                nbr_boxes += target_true['labels'].shape[0]\n",
    "\n",
    "                results.extend(evaluate_sample(target_pred, target_true))\n",
    "\n",
    "    results = sorted(results, key=lambda k: k['score'], reverse=True)\n",
    "\n",
    "    # compute precision and recall to calculate mAP\n",
    "\n",
    "    ## YOUR CODE HERE\n",
    "\n",
    "    return auc(recall, precision)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4quQZewevyvp"
   },
   "source": [
    "## Step 4. Train functions (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNBQcU_cSgts"
   },
   "source": [
    "Now define the functions for training the model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "26JW24UJSmaq"
   },
   "source": [
    "def train_one_epoch(model, train_dataloader, optimizer, device):\n",
    "    # YOUR CODE HERE\n",
    "    # TRAIN YOUR MODEL ON THE train_dataloader\n",
    "    pass\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, optimizer, device, n_epochs=10):\n",
    "    for epoch in range(n_epochs):\n",
    "        model.eval()\n",
    "        test_auc = evaluate(model, val_dataloader, device=device)\n",
    "        print(\"AUC ON TEST: {:.4f}\".format(test_auc))\n",
    "        model.train()\n",
    "        train_one_epoch(model, train_dataloader, optimizer, device=device)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a5zi3LMUwXao"
   },
   "source": [
    "## Step 5. Train model (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOIm5e6TT7Pm"
   },
   "source": [
    "Train the model for object detection on a training dataset and achieve a PR-AUC of at least 0.91 on a test dataset. You can use models from `torchvision`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AJ0Bi_JUHoe"
   },
   "source": [
    "It is mandatory to use augmentation for training to achieve the desired result on the test. Use the `torchvision.transforms` module or the [albumentations](https://albumentations.ai/) library. The latter library is especially convenient since it can calculate the new coordinates of bounding boxes itself after image transformations. We advise you to pay attention to this [tutorial](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/). Please note that the code written in the dataset above is only correct if you are using `albumentations`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OfX1ayfrTgNO"
   },
   "source": [
    "train_transform = # YOUR CODE FOR AUGMENTATIONS\n",
    "val_transform = # YOUR CODE FOR VALIDATION AUGMENTATIONS\n",
    "# HINT: TRAIN TRANSFORM OBVIOUSLY SHOULD BE HARDER THAN THOSE FOR VALIDATION\n",
    "\n",
    "train_dataset = FruitDataset(\"./train_zip/train\", transform=train_transform)\n",
    "val_dataset = FruitDataset(\"./test_zip/test\", transform=val_transform)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rN_qlKqnwidK"
   },
   "source": [
    "model = # YOUR CODE, CREATE MODEL FOR OBJECT DETECTION\r\n",
    "# HINT: YOU CAN USE torchvision.models AND torchvision.models.detection\r\n",
    "# READ OFFICIAL DOCS FOR MORE INFO\r\n",
    "\r\n",
    "optimizer = # SELECT YOUR OPTIMIZER\r\n",
    "train_dataloader = # CREATE YOUR DATALOADER, SELECT APPROPRIATE batch_size\r\n",
    "val_dataloader = # CREATE VALIDATION DATALOADER\r\n",
    "n_epochs = # SELECT APPROPRIZTE NUMBER OF EPOCHS\r\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\r\n",
    "\r\n",
    "train(model, train_dataloader, val_dataloader, optimizer, device, n_epochs)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c60A2kk0S8R9"
   },
   "source": [
    "Output the final quality of the model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q0pjpdNGS3n-"
   },
   "source": [
    "evaluate(model, val_dataloader, criterion)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RdAOThYUS-91"
   },
   "source": [
    "Draw predicted bounding boxes for any two images from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qCrtr-x9TA8G"
   },
   "source": [
    "image, labels = next(iter(train_dataset))\r\n",
    "pred = model(image.unsqueeze(0).to(device))[0]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iUQq3OxvTGti"
   },
   "source": [
    "from PIL import ImageDraw\r\n",
    "\r\n",
    "image = torchvision.transform.ToPILImage()(image)\r\n",
    "draw = ImageDraw.Draw(image)\r\n",
    "for box in labels['boxes']:\r\n",
    "    draw.rectangle([(box[0], box[1]), (box[2], box[3])])\r\n",
    "    \r\n",
    "for box in pred['boxes']:\r\n",
    "    draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline='red')\r\n",
    "image"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}