{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffuHOST6CsLm"
   },
   "source": [
    "# Generating names with recurrent neural networks\n",
    "\n",
    "In this programming assignment you'll find yourself delving into the heart (and other intestines) of recurrent neural networks on a class of toy problems.\n",
    "\n",
    "Struggle to find a name for the variable? Let's see how you'll come up with a name for your son/daughter. Surely no human has expertize over what is a good child name, so let us train RNN instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WulV-Skdzc8Y"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "shred -u setup_colab.py\n",
    "\n",
    "wget https://raw.githubusercontent.com/hse-aml/intro-to-dl-pytorch/main/utils/setup_colab.py -O setup_colab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NBDyjj2ezc8Y"
   },
   "outputs": [],
   "source": [
    "import setup_colab\n",
    "\n",
    "setup_colab.setup_week05()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dloEnPemCsLt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOVCrUnHQmJT"
   },
   "source": [
    "### Fill in your Coursera token and email\n",
    "To successfully submit your answers to our grader, please fill in your Coursera submission token and email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAeDUQxwQnUa"
   },
   "outputs": [],
   "source": [
    "import grading\n",
    "\n",
    "grader = grading.Grader(\n",
    "    assignment_key=\"cULEpp2NEeemQBKZKgu93A\",\n",
    "    all_parts=[\"pttMO\", \"uly0D\", \"mf20L\", \"zwTu9\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnklRR5-QnXO"
   },
   "outputs": [],
   "source": [
    "# token expires every 30 min\n",
    "COURSERA_TOKEN = \"### YOUR TOKEN HERE ###\"\n",
    "COURSERA_EMAIL = \"### YOUR EMAIL HERE ###\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTT2uW5xCsLu"
   },
   "source": [
    "# Load data\n",
    "The dataset contains ~8k names from different cultures, all in latin transcript.\n",
    "\n",
    "This notebook has been designed so as to allow you to quickly swap names for something similar: deep learning article titles, IKEA furniture, pokemon names, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.701832Z",
     "start_time": "2018-08-13T20:26:42.697766Z"
    },
    "id": "dhFyOX6PCsLv"
   },
   "outputs": [],
   "source": [
    "start_token = \" \"  # so that the network knows that we're generating a first token\n",
    "\n",
    "# this is the token for padding,\n",
    "# we will add fake pad token at the end of names \n",
    "# to make them of equal size for further batching\n",
    "pad_token = \"#\"\n",
    "\n",
    "with open(\"names.txt\") as f:\n",
    "    names = f.read()[:-1].split('\\n')\n",
    "    names = [start_token + name for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.707885Z",
     "start_time": "2018-08-13T20:26:42.703302Z"
    },
    "id": "Kf43mc6CCsLv"
   },
   "outputs": [],
   "source": [
    "print('number of samples:', len(names))\n",
    "for x in names[::1000]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.857411Z",
     "start_time": "2018-08-13T20:26:42.709371Z"
    },
    "id": "72rNxCG9CsLv"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = max(map(len, names))\n",
    "print(\"max length:\", MAX_LENGTH)\n",
    "\n",
    "plt.title('Sequence length distribution')\n",
    "plt.hist(list(map(len, names)), bins=25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3VEZIuHCsLv"
   },
   "source": [
    "# Text processing\n",
    "\n",
    "First we need to collect a \"vocabulary\" of all unique tokens i.e. unique characters. We can then encode inputs as a sequence of character ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.864592Z",
     "start_time": "2018-08-13T20:26:42.858725Z"
    },
    "id": "CLCx5pkcCsLw"
   },
   "outputs": [],
   "source": [
    "tokens = ### YOUR CODE HERE: all unique characters in the dataset ###\n",
    "\n",
    "num_tokens = len(tokens)\n",
    "print ('num_tokens = ', num_tokens)\n",
    "\n",
    "assert 50 < num_tokens < 60, \"Names should contain within 50 and 60 unique tokens depending on encoding\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61QZczNkCsLw"
   },
   "source": [
    "### Cast everything from symbols into identifiers\n",
    "\n",
    "Instead of symbols we'll feed our recurrent neural network with ids of characters from our dictionary.\n",
    "\n",
    "To create such dictionary, let's assign `token_to_id`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.870330Z",
     "start_time": "2018-08-13T20:26:42.866135Z"
    },
    "id": "9AxRKAp0CsLx"
   },
   "outputs": [],
   "source": [
    "token_to_id =### YOUR CODE HERE: create a dictionary of {symbol -> its  index in tokens}\n",
    "\n",
    "assert len(tokens) == len(token_to_id), \"dictionaries must have same size\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.875943Z",
     "start_time": "2018-08-13T20:26:42.871834Z"
    },
    "id": "vJ4tU0V-CsLx"
   },
   "outputs": [],
   "source": [
    "def to_matrix(lines, max_len=None, pad=token_to_id[pad_token], dtype='int32', batch_first = True):\n",
    "    \"\"\"Casts a list of names into rnn-digestable matrix\"\"\"\n",
    "    \n",
    "    max_len = max_len or max(map(len, lines))\n",
    "    lines_ix = np.zeros([len(lines), max_len], dtype) + pad\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        line_ix = [token_to_id[c] for c in lines[i]]\n",
    "        lines_ix[i, :len(line_ix)] = line_ix\n",
    "        \n",
    "    if not batch_first: # convert [batch, time] into [time, batch]\n",
    "        lines_ix = np.transpose(lines_ix)\n",
    "\n",
    "    return lines_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:42.883107Z",
     "start_time": "2018-08-13T20:26:42.877186Z"
    },
    "id": "kyBkrX0BCsLx"
   },
   "outputs": [],
   "source": [
    "# Example: cast 4 random names to padded matrices (so that we can easily batch them)\n",
    "print('\\n'.join(names[::2000]))\n",
    "print(to_matrix(names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rn0in_1ICsLx"
   },
   "source": [
    "# Defining a recurrent neural network\n",
    "\n",
    "We can rewrite recurrent neural network as a consecutive application of dense layer to input $x_t$ and previous rnn state $h_t$. This is exactly what we're gonna do now.\n",
    "<img src=\"images/rnn.png\" width=600>\n",
    "\n",
    "Since we're training a language model, there should also be:\n",
    "* An embedding layer that converts character id x_t to a vector.\n",
    "* An output layer that predicts probabilities of next phoneme based on h_t+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fq_BI6hpUPqS"
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:44.044903Z",
     "start_time": "2018-08-13T20:26:44.041084Z"
    },
    "id": "Yq0HH3UfCsLy"
   },
   "outputs": [],
   "source": [
    "class CharRNNCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Implement the scheme above as torch module\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_tokens=len(tokens), embedding_size=16, rnn_num_units=64):\n",
    "        super().__init__()\n",
    "        self.num_units = rnn_num_units\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_tokens, embedding_size)\n",
    "        self.rnn_update = nn.Linear(embedding_size + rnn_num_units, rnn_num_units)\n",
    "        self.rnn_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
    "        \n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"\n",
    "        This method computes h_next(x, h_prev) and log P(x_next | h_next)\n",
    "        We'll call it repeatedly to produce the whole sequence.\n",
    "        \n",
    "        :param x: batch of character ids, int64[batch_size]\n",
    "        :param h_prev: previous rnn hidden states, float32 matrix [batch, rnn_num_units]\n",
    "        \"\"\"\n",
    "        # get vector embedding of x\n",
    "        x_emb = ### YOUR CODE HERE ###\n",
    "        # compute next hidden state using self.rnn_update\n",
    "        # hint: use torch.cat(..., dim=...) for concatenation\n",
    "\n",
    "        h_next = ### YOUR CODE HERE ###\n",
    "        \n",
    "        assert h_next.size() == h_prev.size()\n",
    "        \n",
    "        #compute logits for next character probs\n",
    "        logits = ### YOUR CODE HERE ###\n",
    "        \n",
    "        return h_next, logits\n",
    "    \n",
    "    def initial_state(self, batch_size):\n",
    "        \"\"\" return rnn state before it processes first input (aka h0) \"\"\"\n",
    "        return torch.zeros(batch_size, self.num_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RukLWQHqCsLy"
   },
   "source": [
    "# RNN: loop\n",
    "\n",
    "Once `rnn_one_step` is ready, let's apply it in a loop over name characters to get predictions -- we will generate names character by character starting with start_token:\n",
    "\n",
    "<img src=\"images/char-nn.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-13T20:26:44.342948Z",
     "start_time": "2018-08-13T20:26:44.056136Z"
    },
    "id": "JetmBlR3CsLy"
   },
   "outputs": [],
   "source": [
    "def rnn_loop(char_rnn, batch_ix):\n",
    "    \"\"\"\n",
    "    Computes logits_seq(next_character) for all time-steps in batch_ix\n",
    "    :param batch_ix: an int32 matrix of shape [batch, time], output of to_matrix(lines)\n",
    "    \"\"\"\n",
    "    batch_size, max_length = batch_ix.size()\n",
    "    hid_state = char_rnn.initial_state(batch_size)\n",
    "    logits_seq = []\n",
    "\n",
    "    for x_t in batch_ix.transpose(0,1):\n",
    "        hid_state, logits = char_rnn(x_t, hid_state)  # <-- here we call your one-step code\n",
    "        logits_seq.append(logits)\n",
    "        \n",
    "    return torch.stack(logits_seq, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EQmar3Z6vzWA"
   },
   "source": [
    "Check that the output of rnn_loop has the right format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pq_nvp_hCsLz"
   },
   "outputs": [],
   "source": [
    "batch_ix = to_matrix(names[:5])\n",
    "batch_ix = torch.tensor(batch_ix, dtype=torch.int64)\n",
    "\n",
    "logits_seq = rnn_loop(CharRNNCell(), batch_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sIE8z_NPvdJs"
   },
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"mf20L\", tuple(logits_seq.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xs6EJtCyCsL0"
   },
   "source": [
    "## Training\n",
    "We train our char-rnn exactly the same way we train any deep learning model, the only difference is that this time we sample strings. \n",
    "\n",
    "To compute the loss in a vectorized manner, we can take `batch_ix[:, 1:]` -- a matrix of token ids shifted 1 step to the left so i-th element is acutally the \"next token\" for i-th prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9MFAe_bCsL1"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from random import sample\n",
    "\n",
    "DEVICE = torch.device('cpu')  # you can change to `cuda`\n",
    "\n",
    "char_rnn = CharRNNCell().to(DEVICE)\n",
    "opt = torch.optim.Adam(char_rnn.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfylXTtzCsL1"
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    # for simplisity we will \n",
    "    batch_ix = to_matrix(sample(names, 32))\n",
    "    batch_ix = torch.tensor(batch_ix, dtype=torch.int64).to(DEVICE)\n",
    "    \n",
    "    # do forward pass\n",
    "    logits_seq = rnn_loop(char_rnn, batch_ix)\n",
    "\n",
    "    # make shifted versions of batch and predictions to compute the loss \n",
    "    predictions_logits = logits_seq[:, :-1]\n",
    "    actual_next_tokens = batch_ix[:, 1:]\n",
    "    \n",
    "    # compute loss\n",
    "    loss = ### YOUR CODE HERE ###\n",
    "    \n",
    "    # train with backprop\n",
    "    ### YOUR CODE HERE ###\n",
    "\n",
    "assert np.mean(history[:10]) > np.mean(history[-10:]), \"RNN didn't converge.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HVv8Wjn6EHm"
   },
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"zwTu9\", int(np.mean(history[:10]) > np.mean(history[-10:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5-v3rfx4TSp"
   },
   "source": [
    "Here we computed loss over all symbols including pad tokens at the end of each name. In practice it would be better to exclude all pad tokens except one for each sequence. We need our model to be able to generate one pad token at the end of the sequence to mark the end of the sequence, but there is no need to generate all next pad tokens (we use them just for \n",
    "convenient data representation). \n",
    "\n",
    "Parameter ignore_index of CrossEntropyLoss allows to do so.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVXLPYYECsL1"
   },
   "source": [
    "## RNN: sampling\n",
    "Once we've trained our network a bit, let's get to actually generating stuff. All we need is the single rnn step function you have defined in char_rnn.forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwI8hhSxCsL1"
   },
   "outputs": [],
   "source": [
    "def generate_sample(char_rnn, seed_phrase=' ', max_length=MAX_LENGTH, temperature=1.0):\n",
    "    '''\n",
    "    The function generates text given a start phrase.\n",
    "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
    "    :param max_length: maximum output length, including seed_phrase\n",
    "    :param temperature: coefficient for sampling.  Higher temperature produces more chaotic outputs,\n",
    "                        smaller temperature converges to the single most likely output\n",
    "    '''\n",
    "    \n",
    "    x_sequence = [token_to_id[token] for token in seed_phrase]\n",
    "    x_sequence = torch.tensor([x_sequence], dtype=torch.int64)\n",
    "    hid_state = char_rnn.initial_state(batch_size=1)\n",
    "    \n",
    "    #feed the seed phrase, if any\n",
    "    for i in range(len(seed_phrase) - 1):\n",
    "        hid_state, _ = char_rnn(x_sequence[:, i], hid_state)\n",
    "    \n",
    "    #start generating\n",
    "    for _ in range(max_length - len(seed_phrase)):\n",
    "        hid_state, logits_next = char_rnn(x_sequence[:, -1], hid_state)\n",
    "        p_next = F.softmax(logits_next / temperature, dim=-1).data.numpy()[0]\n",
    "        \n",
    "        # sample next token and push it back into x_sequence\n",
    "        next_ix = np.random.choice(num_tokens,p=p_next)\n",
    "        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
    "        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
    "        \n",
    "    return ''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHdkvurmCsL2"
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(generate_sample(char_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2RnK9FZACsL2"
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    print(generate_sample(char_rnn, seed_phrase=' Trump'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbukAsjoCsL8"
   },
   "source": [
    "## More hight-level implementation\n",
    "What we just did is a manual low-level implementation of RNN. While it's cool, we guess you won't like the idea of re-writing it from scratch on every occasion.\n",
    "\n",
    "As you might have guessed, torch has a solution for this. To be more specific, there are two options:\n",
    "\n",
    "`nn.RNNCell(emb_size, rnn_num_units)` - implements a single step of RNN just like you did. Basically concat-linear-tanh\n",
    "`nn.RNN(emb_size, rnn_num_units)` - implements the whole rnn_loop for you.\n",
    "There's also `nn.LSTMCell` vs `nn.LSTM`, `nn.GRUCell` vs `nn.GRU`, etc. etc.\n",
    "\n",
    "In this example we'll rewrite the char_rnn and rnn_loop using high-level rnn API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UacxOUHMCsL9"
   },
   "outputs": [],
   "source": [
    "class CharRNNLoop(nn.Module):\n",
    "    def __init__(self, num_tokens=num_tokens, emb_size=16, rnn_num_units=64):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.emb = nn.Embedding(num_tokens, emb_size)\n",
    "        self.rnn = nn.RNN(emb_size, rnn_num_units, batch_first=True)\n",
    "        self.hid_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Computes log P(next_character) for all time-steps in x\n",
    "        :param x: an int32 matrix of shape [batch, time], output of to_matrix(lines)\n",
    "        :output next_logp: a float32 tensor [batch, time, dictionary_size]\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jouqgGYkCG3z"
   },
   "source": [
    "Train the model using the same training code and check that it works very similar to our hand-written RNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMQSqcfRBPLD"
   },
   "outputs": [],
   "source": [
    "model = CharRNNLoop().to(DEVICE)\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "history_high = [] # put the history in this variable for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RiK3feiNBdTk"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n",
    "assert np.mean(history_high[:10]) > np.mean(history_high[-10:]), \"RNN didn't converge.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpWe48ahB0VE"
   },
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"pttMO\", int(np.mean(history_high[:10]) > np.mean(history_high[-10:])))\n",
    "grader.set_answer(\"uly0D\", len(set([generate_sample(char_rnn, ' Sad') for _ in range(25)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HOFdJpvqzc8i"
   },
   "outputs": [],
   "source": [
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HnOnFHU-CsL9"
   },
   "source": [
    "Here's another example with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3sn0jpUCsL9"
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class CharLSTMCell(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements something like CharRNNCell, but with LSTM\n",
    "    \"\"\"\n",
    "    def __init__(self, num_tokens=len(tokens), embedding_size=16, rnn_num_units=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_units = rnn_num_units\n",
    "        self.emb = nn.Embedding(num_tokens, embedding_size)\n",
    "        self.lstm = nn.LSTMCell(embedding_size, rnn_num_units)\n",
    "        self.rnn_to_logits = nn.Linear(rnn_num_units, num_tokens)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        (prev_h, prev_c) = prev_state\n",
    "        (next_h, next_c) = self.lstm(self.emb(x), (prev_h, prev_c))\n",
    "        logits = self.rnn_to_logits(next_h)\n",
    "        \n",
    "        return (next_h, next_c), logits\n",
    "    \n",
    "    def initial_state(self, batch_size):\n",
    "        \"\"\" LSTM has two state variables, cell and hid \"\"\"\n",
    "        return torch.zeros(batch_size, self.num_units), torch.zeros(batch_size, self.num_units)\n",
    "    \n",
    "char_lstm = CharLSTMCell()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMRUeIzNCsL8"
   },
   "source": [
    "# Try it out!\n",
    "\n",
    "__Disclaimer:__ This part of assignment is entirely optional. You won't receive bonus points for it. However, it's a fun thing to do. Please share your results on course forums.\n",
    "\n",
    "You've just implemented a recurrent language model that can be tasked with generating any kind of sequence, so there's plenty of data you can try it on:\n",
    "\n",
    "* Novels/poems/songs of your favorite author\n",
    "* News titles/clickbait titles\n",
    "* Source code of Linux or Tensorflow\n",
    "* Molecules in [smiles](https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system) format\n",
    "* Melody in notes/chords format\n",
    "* IKEA catalog titles\n",
    "* Pokemon names\n",
    "* Cards from Magic, the Gathering / Hearthstone\n",
    "\n",
    "If you're willing to give it a try, here's what you wanna look at:\n",
    "* Current data format is a sequence of lines, so a novel can be formatted as a list of sentences. Alternatively, you can change data preprocessing altogether.\n",
    "* While some datasets are readily available, others can only be scraped from the web. Try `Selenium` or `Scrapy` for that.\n",
    "* Make sure MAX_LENGTH is adjusted for longer datasets. \n",
    "* More complex tasks require larger RNN architecture, try more neurons or several layers. It would also require more training iterations.\n",
    "* Long-term dependencies in music, novels or molecules are better handled with LSTM or GRU\n",
    "\n",
    "__Good hunting!__"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "week05_generating_names_with_rnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}