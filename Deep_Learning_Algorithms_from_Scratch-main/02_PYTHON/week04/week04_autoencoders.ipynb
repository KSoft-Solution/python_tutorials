{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yH4mcK4lnpyy"
   },
   "source": [
    "# Denoising Autoencoders And Where To Find Them\n",
    "\n",
    "In this programming assignment we're going to train deep autoencoders and apply them to faces and similar images search.\n",
    "\n",
    "Our new test subjects are human faces from the [lfw dataset](http://vis-www.cs.umass.edu/lfw/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkOrpfaD4kr2"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "shred -u setup_colab.py\n",
    "\n",
    "wget https://raw.githubusercontent.com/hse-aml/intro-to-dl-pytorch/main/utils/setup_colab.py -O setup_colab.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MyYXfjQp4kr3"
   },
   "outputs": [],
   "source": [
    "import setup_colab\n",
    "\n",
    "setup_colab.setup_week04()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxHs2VRMnpy1"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import download_utils\n",
    "from lfw_dataset import load_lfw_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4-8s1yoXWa98"
   },
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGNzUvfw-dh0"
   },
   "source": [
    "### Fill in your Coursera token and email\n",
    "To successfully submit your answers to our grader, please fill in your Coursera submission token and email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LiZcOVZx-VvW"
   },
   "outputs": [],
   "source": [
    "import grading\n",
    "\n",
    "grader = grading.Grader(\n",
    "    assignment_key=\"9TShnp1JEeeGGAoCUnhvuA\",\n",
    "    all_parts=[\"FtBSK\", \"83Glu\", \"fnM1K\", \"T5tJ7\", \"UF05M\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ly7qBIys-jsY"
   },
   "outputs": [],
   "source": [
    "# token expires every 30 min\n",
    "COURSERA_TOKEN = \"### YOUR TOKEN HERE ###\"\n",
    "COURSERA_EMAIL = \"### YOUR EMAIL HERE ###\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJvWmC6hnpy1"
   },
   "source": [
    "# Load dataset\n",
    "Dataset was downloaded for you. Relevant links (just in case):\n",
    "- http://www.cs.columbia.edu/CAVE/databases/pubfig/download/lfw_attributes.txt\n",
    "- http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz\n",
    "- http://vis-www.cs.umass.edu/lfw/lfw.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQQYWym2npy2"
   },
   "outputs": [],
   "source": [
    "# we downloaded them for you, just link them here\n",
    "download_utils.download_week_4_resources('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5La7VfHnpy2"
   },
   "outputs": [],
   "source": [
    "# load images\n",
    "X, attr = load_lfw_dataset(use_raw=True, dimx=32, dimy=32)\n",
    "IMG_SHAPE = X.shape[1:]\n",
    "\n",
    "# center images\n",
    "X = X.astype('float32') / 255.0 - 0.5\n",
    "X = np.transpose(X, (0, 3, 1, 2))\n",
    "\n",
    "# split\n",
    "X_train, X_test = train_test_split(X, test_size=0.1, random_state=42)\n",
    "X_train = torch.from_numpy(X_train)\n",
    "X_test = torch.from_numpy(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O2M84a4snpy2"
   },
   "outputs": [],
   "source": [
    "def show_image(x):\n",
    "    x = x.permute(1, 2, 0)\n",
    "    plt.imshow(torch.clamp(x + 0.5, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXBJezNVnpy3"
   },
   "outputs": [],
   "source": [
    "show_image(X_train[3210])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umOjYfkSnpy3"
   },
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3f48qrwlnpy3"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zl420k5Nnpy3"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataset = TensorDataset(X_train)\n",
    "test_dataset = TensorDataset(X_test)\n",
    "\n",
    "\n",
    "# try changing arguments of DataLoaders to speed up processing\n",
    "train_dataloader = DataLoader(train_dataset, BATCH_SIZE, num_workers=4)\n",
    "test_dataloader = DataLoader(test_dataset, BATCH_SIZE, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lkyv9VdYnpy4"
   },
   "source": [
    "# Autoencoder architecture\n",
    "\n",
    "<img src=\"images/autoencoder.png\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4oy2TiUnpy4"
   },
   "source": [
    "# First step: PCA\n",
    "\n",
    "Principial Component Analysis is a popular dimensionality reduction method. \n",
    "\n",
    "Under the hood, PCA attempts to decompose object-feature matrix $X$ into two smaller matrices: $W$ and $\\hat W$ minimizing _mean squared error_:\n",
    "\n",
    "$$\\|(X W) \\hat{W} - X\\|^2_2 \\to_{W, \\hat{W}} \\min$$\n",
    "- $X \\in \\mathbb{R}^{n \\times m}$ - object matrix (**centered**);\n",
    "- $W \\in \\mathbb{R}^{m \\times d}$ - matrix of direct transformation;\n",
    "- $\\hat{W} \\in \\mathbb{R}^{d \\times m}$ - matrix of reverse transformation;\n",
    "- $n$ samples, $m$ original dimensions and $d$ target dimensions;\n",
    "\n",
    "In geometric terms, we want to find d axes along which most of variance occurs. The \"natural\" axes, if you wish.\n",
    "\n",
    "<img src=\"images/pca.png\" style=\"width:30%\">\n",
    "\n",
    "\n",
    "PCA can also be seen as a special case of an autoencoder.\n",
    "\n",
    "* __Encoder__: X -> Dense(d units) -> code\n",
    "* __Decoder__: code -> Dense(m units) -> X\n",
    "\n",
    "Where Dense is a fully-connected layer with linear activaton:   $f(X) = W \\cdot X + \\vec b $\n",
    "\n",
    "\n",
    "Note: the bias term in those layers is responsible for \"centering\" the matrix i.e. substracting mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YuutYvu7npy4"
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    \n",
    "    # note that typing is not necessary\n",
    "    # but it makes life much easier\n",
    "    def __init__(self, image_shape: Tuple[int, ...], code_size: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert len(image_shape) == 3\n",
    "        \n",
    "        self.image_shape = image_shape\n",
    "        self.code_size = code_size\n",
    "        \n",
    "        input_dim = image_shape[0] * image_shape[1] * image_shape[2]\n",
    "        \n",
    "        # Initialize `self.encoder` and `self.decoder` as simple Linear layers\n",
    "        ### YOUR CODE HERE ###\n",
    "        \n",
    "    def forward(self, image: torch.Tensor):\n",
    "        code = self.encode(image)\n",
    "        return self.decode(code)\n",
    "    \n",
    "    def encode(self, image: torch.Tensor):\n",
    "        image = image.flatten(start_dim=1)\n",
    "        return self.encoder(image)\n",
    "    \n",
    "    def decode(self, code: torch.Tensor):\n",
    "        image = self.decoder(code)\n",
    "        image = image.view(-1, *self.image_shape)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhkAS_38npy4"
   },
   "source": [
    "## Init train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8ieWv1Snpy4"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCH = 15\n",
    "DEVICE = torch.device('cuda')\n",
    "\n",
    "autoencoder = AutoEncoder((3, 32, 32), 100).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YM8Liadnpy5"
   },
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "\n",
    "# TRAIN MODEL\n",
    "### YOUR CODE HERE ### \n",
    "# Feel free to use train loop from notebook of previous week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HGv2Y1XkW72G"
   },
   "outputs": [],
   "source": [
    "assert min(history['loss']) < 0.0036, 'Loss is too high!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkAiXNVI5x4P"
   },
   "source": [
    "### Evaluate model on test data\n",
    "\n",
    "Calculate loss and vizualize some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2JYf28xnpy5"
   },
   "outputs": [],
   "source": [
    "def visualize(image, autoencoder):\n",
    "    \"\"\"\n",
    "    Draws original, encoded and decoded images\n",
    "    \n",
    "    \n",
    "    NOTE\n",
    "    Expect image.shape == [3, 32, 32]\n",
    "    \"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        code = autoencoder.encode(image.unsqueeze(0))\n",
    "        reco = autoencoder.decode(code).squeeze(0)\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"Original\")\n",
    "    show_image(image.squeeze(0))\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"Code\")\n",
    "    plt.imshow(code.view(code.shape[-1] // 2, -1))\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"Reconstructed\")\n",
    "    show_image(reco)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18kxIS6rpUeF"
   },
   "outputs": [],
   "source": [
    "autoencoder = autoencoder.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pPwGJCVNnpy5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    image, = test_dataset[i]\n",
    "    visualize(image, autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"T5tJ7\", min(history['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9uHQ2SSnpy6"
   },
   "source": [
    "# Going deeper: convolutional autoencoder\n",
    "\n",
    "PCA is neat but surely we can do better. This time we want you to build a deep convolutional autoencoder by... stacking more layers.\n",
    "\n",
    "## Encoder\n",
    "\n",
    "The **encoder** part is pretty standard, we stack convolutional and pooling layers and finish with a dense layer to get the representation of desirable size (`code_size`).\n",
    "\n",
    "We recommend to use `nn.ELU'` for all convolutional and dense layers.\n",
    "\n",
    "We recommend to repeat (conv, pool) 4 times with kernel size (3, 3) (don't forget to add padding=1) and the following numbers of output channels: `32, 64, 128, 256`.\n",
    "\n",
    "Remember to flatten (`nn.Flatten()`) output before adding the last dense layer!\n",
    "\n",
    "## Decoder\n",
    "\n",
    "For **decoder** we will use so-called \"transpose convolution\". \n",
    "\n",
    "Traditional convolutional layer takes a patch of an image and produces a number (patch -> number). In \"transpose convolution\" we want to take a number and produce a patch of an image (number -> patch). We need this layer to \"undo\" convolutions in encoder. We had a glimpse of it during week 3 (watch [this video](https://www.coursera.org/learn/intro-to-deep-learning-mds/lecture/auRqf/a-glimpse-of-other-computer-vision-tasks) starting at 5:41).\n",
    "\n",
    "Here's how \"transpose convolution\" works:\n",
    "<img src=\"images/transpose_conv.jpg\" style=\"width:60%\">\n",
    "In this example we use a stride of 2 to produce 4x4 output, this way we \"undo\" pooling as well. Another way to think about it: we \"undo\" convolution with stride 2 (which is similar to conv + pool).\n",
    "\n",
    "You can add \"transpose convolution\" layer in PyTorch like this:\n",
    "```python\n",
    "nn.ConvTranspose2d(...)\n",
    "```\n",
    "\n",
    "Our decoder starts with a dense layer to \"undo\" the last layer of encoder. Remember to reshape its output to \"undo\" `nn.Flatten()` in encoder.\n",
    "\n",
    "Now we're ready to undo (conv, pool) pairs. For this we need to stack 4 `nn.ConvTranspose2d` layers with the following numbers of output channels: `128, 64, 32, 3`. Each of these layers will learn to \"undo\" (conv, pool) pair in encoder. For the last `nn.ConvTranspose2d` layer don't use any activation because that is our final image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPQKPHgznpy6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's play around with transpose convolution on examples first\n",
    "def test_conv2d_transpose(img_size, filter_size, padding=0):\n",
    "    print(\"Transpose convolution test for img_size={}, filter_size={}:\".format(img_size, filter_size))\n",
    "    \n",
    "    x = torch.arange(img_size ** 2).view(1, 1, img_size, img_size).float()\n",
    "    \n",
    "    conv_transpose = nn.ConvTranspose2d(\n",
    "        in_channels=1, out_channels=1,\n",
    "        kernel_size=(filter_size, filter_size),\n",
    "        stride=2, padding=padding, bias=False\n",
    "    )\n",
    "    nn.init.ones_(conv_transpose.weight)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        result = conv_transpose(x)\n",
    "\n",
    "    print(\"input:\")\n",
    "    print(x[0, 0, :, :])\n",
    "    print(\"filter:\")\n",
    "    print(conv_transpose.weight[:, :, 0, 0])\n",
    "    print(\"output:\")\n",
    "    print(result[0, 0, :, :])\n",
    "        \n",
    "test_conv2d_transpose(img_size=2, filter_size=2)\n",
    "test_conv2d_transpose(img_size=2, filter_size=3)\n",
    "test_conv2d_transpose(img_size=2, filter_size=3, padding=1)\n",
    "test_conv2d_transpose(img_size=4, filter_size=2)\n",
    "test_conv2d_transpose(img_size=4, filter_size=3)\n",
    "test_conv2d_transpose(img_size=4, filter_size=3, padding=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jc6CT7Pknpy7"
   },
   "outputs": [],
   "source": [
    "encode_block = lambda i, o: nn.Sequential(\n",
    "    nn.Conv2d(i, o, kernel_size=3, padding=1),\n",
    "    nn.ELU(),\n",
    "    nn.MaxPool2d(2)\n",
    ")\n",
    "\n",
    "decoder_block = lambda i, o, k, s, p, a: nn.Sequential(\n",
    "    nn.ConvTranspose2d(i, o, k, s, p),\n",
    "    nn.ELU() if a else nn.Identity()\n",
    ")\n",
    "\n",
    "class DeepAutoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, image_shape, code_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        assert image_shape == (3, 32, 32)\n",
    "        \n",
    "        # Initialize `self.encoder` with `encode_block` blocks\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(256 * 2 * 2, code_size)\n",
    "        )\n",
    "        self.reprojection = nn.Linear(code_size, 256 * 2 * 2)\n",
    "\n",
    "        # Initialize `self.decoder` with `decoder_block` blocks\n",
    "        ### YOUR CODE HERE ###\n",
    "\n",
    "    \n",
    "    def forward(self, image):\n",
    "        code = self.encode(image)\n",
    "        recon = self.decode(code)\n",
    "        return recon\n",
    "    \n",
    "    def encode(self, image):\n",
    "        output = self.encoder(image)\n",
    "        code = self.projection(output)\n",
    "        return code\n",
    "    \n",
    "    def decode(self, code):\n",
    "        output = self.reprojection(code)\n",
    "        output = output.view(-1, 256, 2, 2)\n",
    "        image = self.decoder(output)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDhimik0npy7"
   },
   "outputs": [],
   "source": [
    "IMG_SHAPE = (3, 32, 32)\n",
    "\n",
    "for code_size in [1, 8, 32, 128, 512]:\n",
    "    test_input = torch.randn(1, *IMG_SHAPE)\n",
    "    model = DeepAutoencoder(IMG_SHAPE, code_size)\n",
    "    print(f\"Testing code size {code_size}\")\n",
    "    \n",
    "    code = model.encode(test_input)\n",
    "    recon_image = model.decode(code)\n",
    "    \n",
    "    assert code.size(-1) == code_size, \"encoder must output a code of required size\"\n",
    "    assert recon_image.shape[1:] == IMG_SHAPE\n",
    "    \n",
    "    assert len([p for p in model.encoder.parameters()]) >= 6, \"encoder must contain at least 3 layers\"\n",
    "    assert len([p for p in model.decoder.parameters()]) >= 6, \"decoder must contain at least 3 layers\"\n",
    "\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_O_cHg5snpy8"
   },
   "outputs": [],
   "source": [
    "def get_num_params(module):\n",
    "    return sum(p.numel() for p in module.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4GTC6fWYnpy8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder = DeepAutoencoder((3, 32, 32), 32)\n",
    "\n",
    "num_encoder_params = get_num_params(autoencoder.encoder)\n",
    "num_decoder_params = get_num_params(autoencoder.decoder)\n",
    "print(num_encoder_params, num_decoder_params)\n",
    "\n",
    "assert np.abs(num_encoder_params - num_decoder_params) < num_encoder_params * 0.1, \\\n",
    "    \"Number of parameters in encoder and decoder should be close\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "notgyoEpnpy8"
   },
   "source": [
    "Convolutional autoencoder training. This will take **1 hour**. You're aiming at ~0.0056 validation MSE and ~0.0054 training MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzn8L74Pnpy8"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCH = 20\n",
    "DEVICE = torch.device('cuda')\n",
    "\n",
    "deep_autoencoder = DeepAutoencoder((3, 32, 32), 32).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(deep_autoencoder.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uh-CYhdnpy8"
   },
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmTf06i-YYXR"
   },
   "outputs": [],
   "source": [
    "assert history['loss'][-1] < 0.0055, 'Loss is too high!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fA_TQ5pP6Ytu"
   },
   "source": [
    "### Evaluate model on test data\n",
    "\n",
    "Calculate loss and vizualize some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cWhvwmHYsfPj"
   },
   "outputs": [],
   "source": [
    "deep_autoencoder = deep_autoencoder.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4Dtfncenpy9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    image, = test_dataset[i]\n",
    "    visualize(image, deep_autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVN912LLnpy9"
   },
   "outputs": [],
   "source": [
    "# save trained weights\n",
    "torch.save(deep_autoencoder.state_dict(), 'deep_autoencoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7SR4fbFzU-d"
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load('deep_autoencoder.pt')\n",
    "deep_autoencoder.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Dcyi-ufnpy9"
   },
   "source": [
    "# Submit to Coursera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhi7XdFN4ksC"
   },
   "outputs": [],
   "source": [
    "test_input = torch.randn(1, *IMG_SHAPE)\n",
    "model = DeepAutoencoder(IMG_SHAPE, 15)\n",
    "\n",
    "code = model.encode(test_input)\n",
    "recon_image = model.decode(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fa45zwen4ksD"
   },
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"FtBSK\", min(history['loss']))\n",
    "grader.set_answer(\"83Glu\", code.size(-1))\n",
    "grader.set_answer(\"fnM1K\", recon_image.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nC00_44g4ksD"
   },
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FJxDZFinpy-"
   },
   "source": [
    "# Denoising Autoencoder\n",
    "\n",
    "It shows you one useful application of autoencoders: denoising. You can run this code and make sure denoising works :) \n",
    "\n",
    "Let's now turn our model into a denoising autoencoder:\n",
    "<img src=\"images/denoising.jpg\" style=\"width:40%\">\n",
    "\n",
    "We'll keep the model architecture, but change the way it is trained. In particular, we'll corrupt its input data randomly with noise before each epoch.\n",
    "\n",
    "There are many strategies to introduce noise: adding gaussian white noise, occluding with random black rectangles, etc. We will add gaussian white noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdnxRE0fnpy-"
   },
   "outputs": [],
   "source": [
    "def apply_gaussian_noise(X, sigma=0.1):\n",
    "    \"\"\"\n",
    "    adds noise from standard normal distribution with standard deviation sigma\n",
    "    :param X: image tensor of shape [batch, 3, height, width]\n",
    "    Returns X + noise.\n",
    "    \"\"\"\n",
    "\n",
    "    ### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pv765d4Enpy-"
   },
   "outputs": [],
   "source": [
    "# noise tests\n",
    "theoretical_std = (X_train[:100].std() ** 2 + 0.5 ** 2) ** .5\n",
    "our_std = apply_gaussian_noise(X_train[:100], sigma=0.5).std()\n",
    "assert abs(theoretical_std - our_std) < 0.01, \\\n",
    "    \"Standard deviation does not match it's required value. Make sure you use sigma as std.\"\n",
    "assert abs(apply_gaussian_noise(X_train[:100], sigma=0.5).mean() - X_train[:100].mean()) < 0.01, \\\n",
    "    \"Mean has changed. Please add zero-mean noise\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GRADED PART, DO NOT CHANGE!\n",
    "grader.set_answer(\"UF05M\", abs(theoretical_std - our_std).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can make submission with answers so far to check yourself at this stage\n",
    "grader.submit(COURSERA_EMAIL, COURSERA_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqQqJA9Knpy-"
   },
   "outputs": [],
   "source": [
    "# test different noise scales\n",
    "plt.subplot(1, 4, 1)\n",
    "show_image(X_train[0])\n",
    "plt.subplot(1, 4, 2)\n",
    "show_image(apply_gaussian_noise(X_train[:1], sigma=0.01)[0])\n",
    "plt.subplot(1, 4, 3)\n",
    "show_image(apply_gaussian_noise(X_train[:1], sigma=0.1)[0])\n",
    "plt.subplot(1, 4, 4)\n",
    "show_image(apply_gaussian_noise(X_train[:1], sigma=0.5)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wHf-xyKnpy_"
   },
   "source": [
    "Training will take **1 hour**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6-avK120840"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCH = 20\n",
    "DEVICE = torch.device('cuda')\n",
    "\n",
    "deep_denoiser_autoencoder = DeepAutoencoder((3, 32, 32), 32).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(deep_denoiser_autoencoder.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONlji6m41Hbu"
   },
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "loss_meter = AverageMeter()\n",
    "\n",
    "for _ in range(NUM_EPOCH):\n",
    "\n",
    "    loss_meter.reset()\n",
    "    for batch in train_dataloader:\n",
    "        image, = batch\n",
    "        noised_image = apply_gaussian_noise(image).to(DEVICE)\n",
    "        image = image.to(DEVICE)\n",
    "\n",
    "        recon_image = deep_denoiser_autoencoder(noised_image)\n",
    "        loss = criterion(image, recon_image)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_meter.update(loss.item())\n",
    "    \n",
    "    history['loss'].append(loss_meter.avg)\n",
    "    print(loss_meter.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5h2eixQZ5UM"
   },
   "outputs": [],
   "source": [
    "assert history['loss'][-1] < 0.0057, 'Loss is too high!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1sortCiv6wCO"
   },
   "source": [
    "### Evaluate model on test data\n",
    "\n",
    "Calculate loss and vizualize some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKeqNrHq2ebv"
   },
   "outputs": [],
   "source": [
    "deep_denoiser_autoencoder = deep_denoiser_autoencoder.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQF1bTIOnpy_",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    image, = test_dataset[i]\n",
    "    noised_image = apply_gaussian_noise(image)\n",
    "    visualize(noised_image, deep_denoiser_autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ed7iFDV6npy_"
   },
   "source": [
    "# Optional: Image retrieval with autoencoders\n",
    "\n",
    "So we've just trained a network that converts image into itself imperfectly. This task is not that useful in and of itself, but it has a number of awesome side-effects. Let's see them in action.\n",
    "\n",
    "First thing we can do is image retrieval aka image search. We will give it an image and find similar images in latent space:\n",
    "\n",
    "<img src=\"images/similar_images.jpg\" style=\"width:60%\">\n",
    "\n",
    "To speed up retrieval process, one should use Locality Sensitive Hashing on top of encoded vectors. This [technique](https://erikbern.com/2015/07/04/benchmark-of-approximate-nearest-neighbor-libraries.html) can narrow down the potential nearest neighbours of our image in latent space (encoder code). We will caclulate nearest neighbours in brute force way for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJLYYf2Ynpy_"
   },
   "outputs": [],
   "source": [
    "# restore trained encoder weights\n",
    "\n",
    "deep_autoencoder = DeepAutoencoder((3, 32, 32), 32)\n",
    "deep_autoencoder.load_state_dict(torch.load('deep_autoencoder.pt', map_location='cpu'))\n",
    "\n",
    "for p in deep_autoencoder.parameters():\n",
    "    p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9qa84zZnpy_"
   },
   "outputs": [],
   "source": [
    "images = X_train\n",
    "\n",
    "# Encode all images\n",
    "### YOUR CODE HERE ###\n",
    "\n",
    "assert len(codes) == len(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YF1RPeGXnpy_"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors.unsupervised import NearestNeighbors\n",
    "nei_clf = NearestNeighbors(metric=\"euclidean\")\n",
    "nei_clf.fit(codes.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HP9DgaC6npy_"
   },
   "outputs": [],
   "source": [
    "def get_similar(image, n_neighbors=5):\n",
    "    assert image.ndim == 3, \"image must be [batch, 3, height, width]\"\n",
    "\n",
    "    code = deep_autoencoder.encode(image.unsqueeze(0)).numpy()\n",
    "    \n",
    "    (distances, ), (idx, ) = nei_clf.kneighbors(code, n_neighbors=n_neighbors)\n",
    "    \n",
    "    return distances, images[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xXn3ZUHnpzA"
   },
   "outputs": [],
   "source": [
    "def show_similar(image):\n",
    "    \n",
    "    distances, neighbors = get_similar(image, n_neighbors=3)\n",
    "    \n",
    "    plt.figure(figsize=[8, 7])\n",
    "    plt.subplot(1, 4, 1)\n",
    "    show_image(image)\n",
    "    plt.title(\"Original image\")\n",
    "    \n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 4, i + 2)\n",
    "        show_image(neighbors[i])\n",
    "        plt.title(\"Dist=%.3f\"%distances[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oRk4rrR-npzA"
   },
   "source": [
    "Cherry-picked examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ll21Q5U1npzA"
   },
   "outputs": [],
   "source": [
    "# smiles\n",
    "show_similar(X_test[247])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzR_mucUnpzA"
   },
   "outputs": [],
   "source": [
    "# ethnicity\n",
    "show_similar(X_test[56])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHW85yPOnpzA",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# glasses\n",
    "show_similar(X_test[63])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "fkxaP_6AnpzA"
   },
   "source": [
    "# Optional: Cheap image morphing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9iYKKL6npzA"
   },
   "source": [
    "We can take linear combinations of image codes to produce new images with decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XzseiHsnpzA"
   },
   "outputs": [],
   "source": [
    "# restore trained encoder weights\n",
    "\n",
    "deep_autoencoder = DeepAutoencoder((3, 32, 32), 32)\n",
    "deep_autoencoder.load_state_dict(torch.load('deep_autoencoder.pt', map_location='cpu'))\n",
    "\n",
    "for p in deep_autoencoder.parameters():\n",
    "    p.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80SH-11MnpzA"
   },
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    image1, image2 = X_test[np.random.randint(0, len(X_test), size=2)]\n",
    "\n",
    "    code1, code2 = deep_autoencoder.encode(torch.stack([image1, image2], dim=0))\n",
    "\n",
    "    plt.figure(figsize=[14, 8])\n",
    "    for i, a in enumerate(torch.linspace(0, 1, steps=7)):\n",
    "\n",
    "        output_code = code1 * (1 - a) + code2 * a\n",
    "        output_image = deep_autoencoder.decode(output_code.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "        plt.subplot(1, 7, i + 1)\n",
    "        show_image(output_image)\n",
    "        plt.title(\"a=%.2f\"%a)\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "aniRd_HAnpzB"
   },
   "source": [
    "That's it!\n",
    "\n",
    "Of course there's a lot more you can do with autoencoders.\n",
    "\n",
    "If you want to generate images from scratch, however, we recommend you Generative Adversarial Networks or GANs."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "week04_autoencoders.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "192px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}